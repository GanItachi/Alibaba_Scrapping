from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time
from sqlalchemy.orm import Session
from api.database import SessionLocal
from api.models import ProduitDetails, Supplier, Review
from api.utils import clean_text, clean_json, clean_price, clean_integer, extract_country_from_url

# üõ†Ô∏è Configuration de Selenium
def setup_driver():
    """Configure et retourne le driver Chrome."""
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")  # Ex√©cuter en mode sans interface
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--window-size=1920,1080")
    return webdriver.Chrome(options=chrome_options)

# ‚úÖ Fonction de scrolling pour charger toute la page
def scroll_page(driver):
    """Fait d√©filer la page pour charger tous les √©l√©ments."""
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)  # Pause pour laisser le temps au contenu de charger
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

# ‚úÖ Scraping des d√©tails du produit
def scrape_product_details(driver):
    """Scrape les informations principales du produit (prix, variations)."""
    product_data = {"pricing": [], "variations": {}, "discount": "Aucune r√©duction"}

    try:
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "product-price")))

        # üîπ V√©rifier s'il y a une promotion active
        try:
            discount_elem = driver.find_element(By.CSS_SELECTOR, ".product-price div.id-text-white")
            product_data["discount"] = clean_text(discount_elem.text)
        except NoSuchElementException:
            pass

        # üîπ R√©cup√©rer les prix et quantit√©s
        try:
            price_items = driver.find_elements(By.CSS_SELECTOR, ".price-item")
            for item in price_items:
                quantity_range = clean_text(item.find_element(By.CLASS_NAME, "quality").text)
                prices = item.find_elements(By.CSS_SELECTOR, ".price span")

                new_price = clean_price(prices[0].text) if len(prices) > 0 else "Non sp√©cifi√©"
                old_price = clean_price(prices[1].text) if len(prices) > 1 else "Non sp√©cifi√©"

                product_data["pricing"].append({
                    "quantity_range": quantity_range,
                    "new_price": new_price,
                    "old_price": old_price
                })
        except NoSuchElementException:
            print("‚ö†Ô∏è Aucun prix trouv√©.")

    except TimeoutException:
        print("‚ùå Impossible de charger les d√©tails du produit.")

    return product_data

# ‚úÖ Scraping des attributs du produit
def scrape_product_attributes(driver):
    """R√©cup√®re les attributs du produit et g√®re le bouton 'Afficher plus'."""
    attributes_data = {}

    try:
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "attribute-info")))

        # üìå V√©rifier et cliquer sur "Afficher plus" si disponible
        try:
            show_more_button = driver.find_element(By.CSS_SELECTOR, ".more-bg a")
            if show_more_button.is_displayed():
                driver.execute_script("arguments[0].click();", show_more_button)
                time.sleep(2)
        except NoSuchElementException:
            pass

        # üîπ Extraire les attributs
        attribute_items = driver.find_elements(By.CLASS_NAME, "attribute-item")
        for item in attribute_items:
            try:
                attribute_name = clean_text(item.find_element(By.CLASS_NAME, "left").text)
                attribute_value = clean_text(item.find_element(By.CLASS_NAME, "right").text)
                attributes_data[attribute_name] = attribute_value
            except NoSuchElementException:
                continue

    except TimeoutException:
        print("‚ùå Impossible de charger la section des attributs.")

    return attributes_data

# ‚úÖ Scraping des avis avec gestion correcte de la pagination
def scrape_reviews(driver):
    """Scrape tous les avis d‚Äôun produit Alibaba en parcourant toutes les pages."""
    reviews_data = []

    try:
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, "module_review")))

        while True:
            reviews = driver.find_elements(By.CSS_SELECTOR, ".review-item")
            for review in reviews:
                try:
                    stars = len(review.find_elements(By.CSS_SELECTOR, ".star-rating-list svg"))
                    comment_elem = review.find_element(By.CSS_SELECTOR, ".review-info")
                    date_elem = review.find_element(By.CSS_SELECTOR, ".date")
                    country_img = review.find_element(By.CSS_SELECTOR, ".avatar-info img")

                    reviews_data.append({
                        "rating": stars,
                        "comment": clean_text(comment_elem.text),
                        "date": clean_text(date_elem.text),
                        "reviewer_country": extract_country_from_url(country_img.get_attribute("src"))
                    })
                except NoSuchElementException:
                    continue

            # üîπ V√©rifier la pagination
            try:
                next_button = driver.find_element(By.CSS_SELECTOR, ".review-pagination .detail-pagination-list button:not(.active)")
                driver.execute_script("arguments[0].click();", next_button)
                time.sleep(2)
            except NoSuchElementException:
                break

    except TimeoutException:
        print("‚ùå Impossible de charger les avis.")

    return reviews_data

# ‚úÖ Scraping des informations du fournisseur (version compl√®te)
def scrape_supplier_info(driver):
    """R√©cup√®re toutes les informations du fournisseur Alibaba."""
    info = {
        "company_name": "Non sp√©cifi√©",
        "company_profile_link": "Non sp√©cifi√©",
        "years_in_business": "Non sp√©cifi√©",
        "location": "Non sp√©cifi√©",
        "store_rating": "Non sp√©cifi√©",
        "on_time_delivery": "Non sp√©cifi√©",
        "response_time": "Non sp√©cifi√©",
        "online_revenue": "Non sp√©cifi√©",
        "main_markets": "Non sp√©cifi√©",
        "staff": "Non sp√©cifi√©",
        "services_offered": [],
        "quality_control": []
    }

    try:
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "company-layout")))

        try:
            company_element = driver.find_element(By.CSS_SELECTOR, ".company-name a")
            info["company_name"] = clean_text(company_element.text)
            info["company_profile_link"] = company_element.get_attribute("href")
        except NoSuchElementException:
            print("‚ö†Ô∏è Nom du fournisseur introuvable.")

    except TimeoutException:
        print("‚ùå Impossible de charger les informations du fournisseur.")

    return info

# ‚úÖ Stockage des donn√©es en base PostgreSQL
def store_product_data(db: Session, product_data, url):
    """Stocke les d√©tails scrapp√©s d'un produit en base de donn√©es."""
    produit = ProduitDetails(
        url=url,
        discount=product_data["details"]["discount"],
        pricing=clean_json(product_data["details"]["pricing"]),
        variations=clean_json(product_data["details"]["variations"]),
        attributes=clean_json(product_data["attributes"])
    )
    db.add(produit)
    db.commit()

# ‚úÖ Scraping du produit complet
def scrape_product(url):
    driver = setup_driver()
    product_data = {}

    try:
        driver.get(url)
        scroll_page(driver)  # üìå Ajout du scroll pour charger toutes les sections
        product_data["details"] = scrape_product_details(driver)
        product_data["attributes"] = scrape_product_attributes(driver)
        product_data["supplier_info"] = scrape_supplier_info(driver)
        product_data["reviews"] = scrape_reviews(driver)

        with SessionLocal() as db:
            store_product_data(db, product_data, url)

    finally:
        driver.quit()

# ‚úÖ Test
if __name__ == "__main__":
    url = "https://www.alibaba.com/product-detail/J-M8-Game-Stick-4K-Video_1600224308009.html"
    scrape_product(url)
