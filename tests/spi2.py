from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from sqlalchemy.orm import Session
from api.database import SessionLocal 
from api.models import Categorie, Produit
import time
import sys
from pathlib import Path
# Ajouter le r√©pertoire racine au PYTHONPATH
root_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(root_dir))
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# ‚úÖ Configuration de Selenium
def setup_driver():
    """Configure et retourne le driver Chrome."""
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")  # Mode sans interface
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-infobars")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=chrome_options)

# ‚úÖ D√©filement pour charger les produits
def wait_for_scroll(driver, last_height, timeout=5):
    """Fait d√©filer la page et attend le chargement du contenu."""
    try:
        WebDriverWait(driver, timeout).until(
            lambda d: d.execute_script("return document.body.scrollHeight") > last_height
        )
        return True
    except TimeoutException:
        return False

# ‚úÖ Scraping des produits d'une cat√©gorie
def scrape_category_products(category_url):
    """Scrape les produits d'une cat√©gorie sur Alibaba et retourne un dictionnaire {nom: lien}."""
    driver = setup_driver()
    products = {}

    try:
        print(f"üîç Chargement de la cat√©gorie : {category_url}")
        driver.get(category_url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "alimod-industry-products-waterfall"))
        )

        last_height = driver.execute_script("return document.body.scrollHeight")
        attempts, max_attempts = 0, 3

        # üîπ Faire d√©filer la page pour charger les produits
        while attempts < max_attempts:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # Ajout d'un d√©lai pour √©viter le chargement incomplet
            
            if wait_for_scroll(driver, last_height, timeout=5):
                last_height = driver.execute_script("return document.body.scrollHeight")
                attempts = 0  # R√©initialisation
            else:
                attempts += 1  # Incr√©mentation si aucun nouveau contenu n'est charg√©

        print(f"‚úÖ D√©filement termin√© pour {category_url}")

        # üîπ Extraire les informations des produits
        product_elements = driver.find_elements(By.CSS_SELECTOR, ".alimod-industry-products-waterfall .hugo4-pc-grid-item")

        for product in product_elements:
            try:
                title_span = product.find_element(By.CSS_SELECTOR, "span[title]")
                title = title_span.get_attribute("title").strip()

                link_element = product.find_element(By.CSS_SELECTOR, "a")
                product_url = link_element.get_attribute("href")

                if title and product_url:
                    products[title] = product_url
            except NoSuchElementException:
                continue

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur dans le scraping de la cat√©gorie : {e}")
    finally:
        driver.quit()

    return products

# ‚úÖ R√©cup√©rer les cat√©gories depuis la base de donn√©es
def get_all_categories(db: Session):
    """R√©cup√®re toutes les cat√©gories enregistr√©es dans la base de donn√©es."""
    return db.query(Categorie).all()

# ‚úÖ Scraper et stocker les produits de toutes les cat√©gories
def spi2():
    """Scrape les produits pour chaque cat√©gorie en base et les stocke dans la table `products`."""
    db = SessionLocal()
    categories = get_all_categories(db)

    if not categories:
        print("‚ö†Ô∏è Aucune cat√©gorie trouv√©e en base.")
        db.close()
        return {}

    for category in categories:
        print(f"\nüîç Scraping des produits pour la cat√©gorie : {category.title}")
        products_data = scrape_category_products(category.link)

        new_products_count = 0
        for title, link in products_data.items():
            # V√©rifier si le produit existe d√©j√† en base
            existing_product = db.query(Produit).filter(Produit.link == link).first()
            if not existing_product:
                new_product = Produit(title=title, link=link, categorie_id=category.id)
                db.add(new_product)
                new_products_count += 1

        db.commit()
        print(f"‚úÖ {new_products_count} nouveaux produits ajout√©s pour la cat√©gorie : {category.title}")

    db.close()
    print("\nüéâ Scraping des produits termin√© !")
    return "Scraping termin√© avec succ√®s !"

# ‚úÖ Test du script
if __name__ == "__main__":
    spi2()
